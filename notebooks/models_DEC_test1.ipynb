{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new compute target...\n",
      "InProgress.......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Workspace\n",
    "\n",
    "  # Load the workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "  # Create a new environment with required packages\n",
    "env = Environment(\"env1-trainingDEC\")\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\"scikit-learn\", \"pandas\", \"numpy\", \"pathlib\"],\n",
    "    pip_packages=[\"azureml-defaults\", \"tensorflow\", \"matplotlib\"])\n",
    "\n",
    "\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your compute target\n",
    "compute_name = \"computetarget1-trainingDEC\"\n",
    "\n",
    "try:\n",
    "    # Check if the compute target already exists\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    # If the compute target doesn't exist, create it\n",
    "    print(\"Creating new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\",  # Change this to the VM size you want to use\n",
    "        max_nodes=4  # Change this to the maximum number of nodes you want to use\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "  # Create a new run configuration\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=\"./\",\n",
    "    script=\"test.ipynb\",\n",
    "    arguments=[],\n",
    "    compute_target=compute_name,\n",
    "    environment=env)\n",
    "\n",
    "# Create a new experiment\n",
    "experiment = Experiment(workspace=ws, name=\"experiment2-trainingDEC\")\n",
    "\n",
    "# Submit the run\n",
    "run = experiment.submit(script_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  General  Type        Common Stock\n",
      "               Exchange          NASDAQ\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 503 entries, AAPL to NWS\n",
      "Data columns (total 40 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   MarketCapitalization        502 non-null    float64\n",
      " 1   DividendYield               502 non-null    float64\n",
      " 2   PERatio                     463 non-null    float64\n",
      " 3   Beta                        499 non-null    float64\n",
      " 4   BookValue                   501 non-null    float64\n",
      " 5   EBITDA                      473 non-null    float64\n",
      " 6   RevenueTTM                  503 non-null    float64\n",
      " 7   GrossProfitTTM              503 non-null    float64\n",
      " 8   OperatingMarginTTM          503 non-null    float64\n",
      " 9   ReturnOnAssetsTTM           503 non-null    float64\n",
      " 10  ReturnOnEquityTTM           503 non-null    float64\n",
      " 11  PriceSalesTTM               503 non-null    float64\n",
      " 12  PriceBookMRQ                503 non-null    float64\n",
      " 13  EnterpriseValue             503 non-null    float64\n",
      " 14  EnterpriseValueRevenue      503 non-null    float64\n",
      " 15  EnterpriseValueEbitda       503 non-null    float64\n",
      " 16  SharesOutstanding           503 non-null    float64\n",
      " 17  PercentInstitutions         503 non-null    float64\n",
      " 18  PayoutRatio                 503 non-null    float64\n",
      " 19  52WeekHigh                  503 non-null    float64\n",
      " 20  52WeekLow                   503 non-null    float64\n",
      " 21  50DayMA                     503 non-null    float64\n",
      " 22  200DayMA                    503 non-null    float64\n",
      " 23  ForwardPE                   503 non-null    float64\n",
      " 24  SharesFloat                 503 non-null    float64\n",
      " 25  PercentInsiders             503 non-null    float64\n",
      " 26  ShortPercent                503 non-null    float64\n",
      " 27  ForwardAnnualDividendRate   503 non-null    float64\n",
      " 28  ForwardAnnualDividendYield  503 non-null    float64\n",
      " 29  TrailingPE                  503 non-null    float64\n",
      " 30  DividendShare               502 non-null    float64\n",
      " 31  EarningsShare               502 non-null    float64\n",
      " 32  EPSEstimateCurrentYear      503 non-null    float64\n",
      " 33  EPSEstimateNextYear         503 non-null    float64\n",
      " 34  EPSEstimateNextQuarter      502 non-null    float64\n",
      " 35  EPSEstimateCurrentQuarter   503 non-null    float64\n",
      " 36  ProfitMargin                503 non-null    float64\n",
      " 37  QuarterlyRevenueGrowthYOY   503 non-null    float64\n",
      " 38  DilutedEpsTTM               503 non-null    float64\n",
      " 39  QuarterlyEarningsGrowthYOY  503 non-null    float64\n",
      "dtypes: float64(40)\n",
      "memory usage: 161.1+ KB\n",
      "MarketCapitalization           1\n",
      "DividendYield                  1\n",
      "PERatio                       40\n",
      "Beta                           4\n",
      "BookValue                      2\n",
      "EBITDA                        30\n",
      "RevenueTTM                     0\n",
      "GrossProfitTTM                 0\n",
      "OperatingMarginTTM             0\n",
      "ReturnOnAssetsTTM              0\n",
      "ReturnOnEquityTTM              0\n",
      "PriceSalesTTM                  0\n",
      "PriceBookMRQ                   0\n",
      "EnterpriseValue                0\n",
      "EnterpriseValueRevenue         0\n",
      "EnterpriseValueEbitda          0\n",
      "SharesOutstanding              0\n",
      "PercentInstitutions            0\n",
      "PayoutRatio                    0\n",
      "52WeekHigh                     0\n",
      "52WeekLow                      0\n",
      "50DayMA                        0\n",
      "200DayMA                       0\n",
      "ForwardPE                      0\n",
      "SharesFloat                    0\n",
      "PercentInsiders                0\n",
      "ShortPercent                   0\n",
      "ForwardAnnualDividendRate      0\n",
      "ForwardAnnualDividendYield     0\n",
      "TrailingPE                     0\n",
      "DividendShare                  1\n",
      "EarningsShare                  1\n",
      "EPSEstimateCurrentYear         0\n",
      "EPSEstimateNextYear            0\n",
      "EPSEstimateNextQuarter         1\n",
      "EPSEstimateCurrentQuarter      0\n",
      "ProfitMargin                   0\n",
      "QuarterlyRevenueGrowthYOY      0\n",
      "DilutedEpsTTM                  0\n",
      "QuarterlyEarningsGrowthYOY     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "meta = pd.read_hdf('/home/groovyjac/projects/Autonomous-Portfolio-Management/main_data_store_JDKv1.h5',\n",
    "                     'stocks/base_fundamentals')\n",
    "\n",
    "print(meta.loc[(\"AAPL\", \"General\", [\"Type\", \"Exchange\"])])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "selected_features = [\n",
    "    'Sector', 'GicSector', 'MarketCapitalization', 'DividendYield', 'PERatio',\n",
    "    'Beta', 'Industry', 'GicGroup', 'GicIndustry', 'GicSubIndustry', 'BookValue',\n",
    "    'EBITDA', 'RevenueTTM', 'GrossProfitTTM', 'OperatingMarginTTM', 'ReturnOnAssetsTTM',\n",
    "    'ReturnOnEquityTTM', 'PriceSalesTTM', 'PriceBookMRQ', 'EnterpriseValue',\n",
    "    'EnterpriseValueRevenue', 'EnterpriseValueEbitda', 'SharesOutstanding',\n",
    "    'PercentInstitutions', 'PayoutRatio', '52WeekHigh', '52WeekLow', '50DayMA',\n",
    "    '200DayMA', 'ForwardPE', 'SharesFloat', 'PercentInsiders',\n",
    "    'ShortPercent', 'ForwardAnnualDividendRate', 'ForwardAnnualDividendYield',\n",
    "    'TrailingPE', 'PriceEarningsRatio', 'DividendShare', 'EarningsShare',\n",
    "    'EPSEstimateCurrentYear', 'EPSEstimateNextYear', 'EPSEstimateNextQuarter',\n",
    "    'EPSEstimateCurrentQuarter', 'ProfitMargin',\n",
    "    'QuarterlyRevenueGrowthYOY', 'DilutedEpsTTM', 'QuarterlyEarningsGrowthYOY'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(meta.loc[idx[:, :, \n",
    "selected_features]])\n",
    "\n",
    "df = df.droplevel(1)[~df.droplevel(1).index.duplicated(keep='first')]\n",
    "\n",
    "df = df.unstack(level=1)\n",
    "\n",
    "df.columns = df.columns.droplevel(0)\n",
    "\n",
    "df = df.drop(['Sector', 'GicSector', 'Industry',\t'GicGroup', 'GicIndustry', 'GicSubIndustry'], axis=1)\n",
    "df = df.astype('float64')\n",
    "df.info()\n",
    "# Check for missing values\n",
    "print(df.isna().sum())\n",
    "# Fill in missing values with the mean of the column\n",
    "df = df.fillna(df.mean())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the model architecture\n",
    "class DECModel(tf.keras.Model):\n",
    "    def __init__(self, n_clusters, input_shape):\n",
    "        super(DECModel, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=input_shape),\n",
    "            tf.keras.layers.Dense(500, activation='relu'),\n",
    "            tf.keras.layers.Dense(500, activation='relu'),\n",
    "            tf.keras.layers.Dense(2000, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation=None)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(2000, activation='relu'),\n",
    "            tf.keras.layers.Dense(500, activation='relu'),\n",
    "            tf.keras.layers.Dense(500, activation='relu'),\n",
    "            tf.keras.layers.Dense(input_shape, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn):\n",
    "        super(DECModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        \n",
    "    def cluster_layer(self, inputs):\n",
    "        q = tf.reduce_sum(tf.square(tf.expand_dims(inputs, axis=1) - self.encoder.weights[-1]) , axis=2)\n",
    "        q = 1.0 / (1.0 + q / self.n_clusters)\n",
    "        q = q ** (2.0 / 3.0)\n",
    "        q = tf.transpose(tf.transpose(q) / tf.reduce_sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "# Define the loss function for the model\n",
    "def dec_loss(x, x_reconstructed, q):\n",
    "    reconstruction_loss = tf.keras.losses.mse(x, x_reconstructed)\n",
    "    clustering_loss = tf.keras.losses.KLD(q, q_target)\n",
    "    return reconstruction_loss + clustering_loss\n",
    "\n",
    "# Create a function to generate target distribution for clustering loss\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 13:09:28.411857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:28.561284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:28.561367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:28.568876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 13:09:28.571337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:28.571471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:28.571521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:29.911275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:29.911591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:29.911665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-05 13:09:29.911725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-05 13:09:29.912384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Sub as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Sub]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[39m=\u001b[39m batch\n\u001b[1;32m     22\u001b[0m x_reconstructed \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> 23\u001b[0m q \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcluster_layer(x)\n\u001b[1;32m     24\u001b[0m q_target \u001b[39m=\u001b[39m target_distribution(q)\n\u001b[1;32m     25\u001b[0m loss \u001b[39m=\u001b[39m dec_loss(x, x_reconstructed, q)\n",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m, in \u001b[0;36mDECModel.cluster_layer\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcluster_layer\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 37\u001b[0m     q \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(tf\u001b[39m.\u001b[39msquare(tf\u001b[39m.\u001b[39;49mexpand_dims(inputs, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mweights[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) , axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     38\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m q \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_clusters)\n\u001b[1;32m     39\u001b[0m     q \u001b[39m=\u001b[39m q \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39m3.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4t/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4t/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Sub as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Sub]"
     ]
    }
   ],
   "source": [
    "# Load the data and scale it\n",
    "data = df.copy()\n",
    "scaled_data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Initialize the model and compile it with the optimizer and loss function\n",
    "model = DECModel(n_clusters=10, input_shape=scaled_data.shape[1])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss_fn=dec_loss)\n",
    "\n",
    "# Create a TensorFlow Dataset object from your scaled dataset\n",
    "batch_size = 32\n",
    "ds = tf.data.Dataset.from_tensor_slices(scaled_data)\n",
    "ds = ds.shuffle(buffer_size=len(scaled_data), reshuffle_each_iteration=True)\n",
    "ds = ds.batch(batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = batch\n",
    "            x_reconstructed = model(x)\n",
    "            q = model.cluster_layer(x)\n",
    "            q_target = target_distribution(q)\n",
    "            loss = dec_loss(x, x_reconstructed, q)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    print(\"Epoch {}: Loss {:.3f}\".format(epoch+1, loss.numpy()))\n",
    "\n",
    "# Save the model\n",
    "model.save('dec_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
