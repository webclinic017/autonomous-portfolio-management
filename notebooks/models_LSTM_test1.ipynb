{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "\n",
    "    prices = pd.read_hdf('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5',\n",
    "                    'stocks/prices/daily').loc[idx[:, '1995':'2023'], ['adjusted_close', 'volume', 'RSI',\n",
    "       'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]\n",
    "    \n",
    "    # prices = (pd.read_hdf(\"home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5\", 'stocks/prices/daily')\n",
    "    #           .loc[idx[:, '2013':'2023'], ['adjusted_close', 'volume']])\n",
    "    prices.index.names = ['ticker', 'date']\n",
    "    n_dates = len(prices.index.unique('date'))\n",
    "    \n",
    "    # ... (include the remaining preprocessing code here)\n",
    "    dollar_vol = (prices.adjusted_close.mul(prices.volume)\n",
    "              .unstack('ticker')\n",
    "              .dropna(thresh=int(.95 * n_dates), axis=1)\n",
    "              .rank(ascending=False, axis=1)\n",
    "              .stack('ticker'))\n",
    "\n",
    "    most_traded = dollar_vol.groupby(level='ticker').mean().nsmallest(500).index\n",
    "\n",
    "    returns = (prices.loc[idx[most_traded, :], 'adjusted_close']\n",
    "            .unstack('ticker')\n",
    "            .pct_change()\n",
    "            .sort_index(ascending=False))\n",
    "    returns.info()\n",
    "\n",
    "    n = len(returns)\n",
    "    T = 21 # days\n",
    "    tcols = list(range(T))\n",
    "    tickers = returns.columns\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(n-T-1):\n",
    "        df = returns.iloc[i:i+T+1]\n",
    "        date = df.index.max()\n",
    "        data = pd.concat([data, \n",
    "                        df.reset_index(drop=True).T\n",
    "                        .assign(date=date, ticker=tickers)\n",
    "                        .set_index(['ticker', 'date'])])\n",
    "    data = data.rename(columns={0: 'label'}).sort_index().dropna()\n",
    "    data.loc[:, tcols[1:]] = (data.loc[:, tcols[1:]].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                    upper=x.quantile(.99))))\n",
    "    data.info()\n",
    "    data.shape\n",
    "\n",
    "    data.to_hdf('lstm_data.h5', 'returns_daily')\n",
    "\n",
    "    with pd.HDFStore('lstm_data.h5') as store:\n",
    "        print(store.info())\n",
    "\n",
    "    prices = (pd.read_hdf(DATA_DIR / 'assets_v1.h5', 'stocks/prices/daily')\n",
    "          .adjusted_close.swaplevel()\n",
    "          .unstack().loc['2005':])\n",
    "    prices.info()\n",
    "\n",
    "    prices.index = pd.to_datetime(prices.index)\n",
    "\n",
    "    returns = (prices\n",
    "            .resample('W')\n",
    "            .last()\n",
    "            .pct_change()\n",
    "            .loc['2006': '2023']\n",
    "            .dropna(axis=1)\n",
    "            .sort_index(ascending=False))\n",
    "    returns.info()\n",
    "\n",
    "    returns.head().append(returns.tail())\n",
    "\n",
    "    n = len(returns)\n",
    "    T = 52 # weeks\n",
    "    tcols = list(range(T))\n",
    "    tickers = returns.columns\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(n-T-1):\n",
    "        df = returns.iloc[i:i+T+1]\n",
    "        date = df.index.max()    \n",
    "        data = pd.concat([data, (df.reset_index(drop=True).T\n",
    "                                .assign(date=date, ticker=tickers)\n",
    "                                .set_index(['ticker', 'date']))])\n",
    "    data.info()\n",
    "\n",
    "    data[tcols] = (data[tcols].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                  upper=x.quantile(.99))))\n",
    "    data = data.rename(columns={0: 'fwd_returns'})\n",
    "    data['label'] = (data['fwd_returns'] > 0).astype(int)\n",
    "    data.shape\n",
    "    data.sort_index().to_hdf('lstm_data.h5', 'returns_weekly')\n",
    "    return prices, returns\n",
    "        \n",
    "    #data.sort_index().to_hdf('lstm_data.h5', 'returns_weekly')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7050 entries, 2022-12-30 to 1995-01-03\n",
      "Columns: 335 entries, MSFT to ROL\n",
      "dtypes: float64(335)\n",
      "memory usage: 18.1+ MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prices, returns \u001b[39m=\u001b[39m preprocess_data()\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     df \u001b[39m=\u001b[39m returns\u001b[39m.\u001b[39miloc[i:i\u001b[39m+\u001b[39mT\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m     date \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mmax()\n\u001b[0;32m---> 44\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([data, \n\u001b[1;32m     45\u001b[0m                     df\u001b[39m.\u001b[39;49mreset_index(drop\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mT\n\u001b[1;32m     46\u001b[0m                     \u001b[39m.\u001b[39;49massign(date\u001b[39m=\u001b[39;49mdate, ticker\u001b[39m=\u001b[39;49mtickers)\n\u001b[1;32m     47\u001b[0m                     \u001b[39m.\u001b[39;49mset_index([\u001b[39m'\u001b[39;49m\u001b[39mticker\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m])])\n\u001b[1;32m     48\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m})\u001b[39m.\u001b[39msort_index()\u001b[39m.\u001b[39mdropna()\n\u001b[1;32m     49\u001b[0m data\u001b[39m.\u001b[39mloc[:, tcols[\u001b[39m1\u001b[39m:]] \u001b[39m=\u001b[39m (data\u001b[39m.\u001b[39mloc[:, tcols[\u001b[39m1\u001b[39m:]]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mclip(lower\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mquantile(\u001b[39m.01\u001b[39m),\n\u001b[1;32m     50\u001b[0m                                                 upper\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mquantile(\u001b[39m.99\u001b[39m))))\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:563\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 563\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:633\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m--> 633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[1;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    636\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:634\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 634\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[1;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    636\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:691\u001b[0m, in \u001b[0;36m_Concatenator._get_concat_axis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mlevels supported only when keys is not None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m     concat_axis \u001b[39m=\u001b[39m _concat_indexes(indexes)\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     concat_axis \u001b[39m=\u001b[39m _make_concat_multiindex(\n\u001b[1;32m    694\u001b[0m         indexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames\n\u001b[1;32m    695\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:709\u001b[0m, in \u001b[0;36m_concat_indexes\u001b[0;34m(indexes)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_concat_indexes\u001b[39m(indexes) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mreturn\u001b[39;00m indexes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mappend(indexes[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/indexes/multi.py:2212\u001b[0m, in \u001b[0;36mMultiIndex.append\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         appended \u001b[39m=\u001b[39m [o\u001b[39m.\u001b[39m_get_level_values(i) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m other]\n\u001b[1;32m   2211\u001b[0m         arrays\u001b[39m.\u001b[39mappend(label\u001b[39m.\u001b[39mappend(appended))\n\u001b[0;32m-> 2212\u001b[0m     \u001b[39mreturn\u001b[39;00m MultiIndex\u001b[39m.\u001b[39;49mfrom_arrays(arrays, names\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnames)\n\u001b[1;32m   2214\u001b[0m to_concat \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values,) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(k\u001b[39m.\u001b[39m_values \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m other)\n\u001b[1;32m   2215\u001b[0m new_tuples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(to_concat)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/indexes/multi.py:489\u001b[0m, in \u001b[0;36mMultiIndex.from_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(arrays[i]) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(arrays[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]):\n\u001b[1;32m    487\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mall arrays must be same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 489\u001b[0m codes, levels \u001b[39m=\u001b[39m factorize_from_iterables(arrays)\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[1;32m    491\u001b[0m     names \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(arr, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:3007\u001b[0m, in \u001b[0;36mfactorize_from_iterables\u001b[0;34m(iterables)\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(iterables) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3004\u001b[0m     \u001b[39m# For consistency, it should return two empty lists.\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []\n\u001b[0;32m-> 3007\u001b[0m codes, categories \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m(factorize_from_iterable(it) \u001b[39mfor\u001b[39;49;00m it \u001b[39min\u001b[39;49;00m iterables))\n\u001b[1;32m   3008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(codes), \u001b[39mlist\u001b[39m(categories)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:3007\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(iterables) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3004\u001b[0m     \u001b[39m# For consistency, it should return two empty lists.\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []\n\u001b[0;32m-> 3007\u001b[0m codes, categories \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m(factorize_from_iterable(it) \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m iterables))\n\u001b[1;32m   3008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(codes), \u001b[39mlist\u001b[39m(categories)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2980\u001b[0m, in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2975\u001b[0m     codes \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mcodes\n\u001b[1;32m   2976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2977\u001b[0m     \u001b[39m# The value of ordered is irrelevant since we don't use cat as such,\u001b[39;00m\n\u001b[1;32m   2978\u001b[0m     \u001b[39m# but only the resulting categories, the order of which is independent\u001b[39;00m\n\u001b[1;32m   2979\u001b[0m     \u001b[39m# from ordered. Set ordered to False as default. See GH #15457\u001b[39;00m\n\u001b[0;32m-> 2980\u001b[0m     cat \u001b[39m=\u001b[39m Categorical(values, ordered\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   2981\u001b[0m     categories \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mcategories\n\u001b[1;32m   2982\u001b[0m     codes \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mcodes\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:441\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mcategories \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    442\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    443\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/algorithms.py:822\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 822\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[1;32m    823\u001b[0m         values,\n\u001b[1;32m    824\u001b[0m         na_sentinel\u001b[39m=\u001b[39;49mna_sentinel_arg,\n\u001b[1;32m    825\u001b[0m         size_hint\u001b[39m=\u001b[39;49msize_hint,\n\u001b[1;32m    826\u001b[0m     )\n\u001b[1;32m    828\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/algorithms.py:575\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mm\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    569\u001b[0m     \u001b[39m# _get_hashtable_algo will cast dt64/td64 to i8 via _ensure_data, so we\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[39m#  need to do the same to na_value. We are assuming here that the passed\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m#  na_value is an appropriately-typed NaT.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[39m# e.g. test_where_datetimelike_categorical\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     na_value \u001b[39m=\u001b[39m iNaT\n\u001b[0;32m--> 575\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    577\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[1;32m    578\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mfactorize(\n\u001b[1;32m    579\u001b[0m     values,\n\u001b[1;32m    580\u001b[0m     na_sentinel\u001b[39m=\u001b[39mna_sentinel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m     ignore_na\u001b[39m=\u001b[39mignore_na,\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/algorithms.py:282\u001b[0m, in \u001b[0;36m_get_hashtable_algo\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mvalues : ndarray\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m values \u001b[39m=\u001b[39m _ensure_data(values)\n\u001b[0;32m--> 282\u001b[0m ndtype \u001b[39m=\u001b[39m _check_object_for_strings(values)\n\u001b[1;32m    283\u001b[0m htable \u001b[39m=\u001b[39m _hashtables[ndtype]\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m htable, values\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/algorithms.py:305\u001b[0m, in \u001b[0;36m_check_object_for_strings\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    299\u001b[0m ndtype \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m ndtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m     \u001b[39m# it's cheaper to use a String Hash Table than Object; we infer\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# including nulls because that is the only difference between\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39m# StringHashTable and ObjectHashtable\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mif\u001b[39;00m lib\u001b[39m.\u001b[39;49minfer_dtype(values, skipna\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    306\u001b[0m         ndtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m \u001b[39mreturn\u001b[39;00m ndtype\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prices, returns = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_hdf('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5',\n",
    "                    'stocks/prices/daily').loc[idx[:, '1995':'2023'], ['adjusted_close', 'volume', 'RSI',\n",
    "       'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3045934 entries, ('AAPL', '1995-01-03') to ('NWS', '2022-12-30')\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   adjusted_close  float64\n",
      " 1   volume          int64  \n",
      " 2   RSI             float64\n",
      " 3   MACD            float64\n",
      " 4   MACD_signal     float64\n",
      " 5   MACD_hist       float64\n",
      " 6   BB_upper        float64\n",
      " 7   BB_middle       float64\n",
      " 8   BB_lower        float64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 221.4+ MB\n"
     ]
    }
   ],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1234633 entries, ('AAPL', '2013-01-02') to ('NWS', '2022-12-30')\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   adjusted_close  1234633 non-null  float64\n",
      " 1   volume          1234633 non-null  int64  \n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 24.2+ MB\n"
     ]
    }
   ],
   "source": [
    "prices = (pd.read_hdf('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5', 'stocks/prices/daily')\n",
    "          .loc[idx[:, '2013':'2023'], ['adjusted_close', 'volume']])\n",
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_data_v2():\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    # Load the dataset provided\n",
    "    dataset = pd.read_hdf('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5',\n",
    "                    'stocks/prices/daily').loc[idx[:, '1995':'2023'], ['adjusted_close', 'volume', 'RSI',\n",
    "                    'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]\n",
    "    dataset.index.names = ['ticker', 'date']\n",
    "    n_dates = len(dataset.index.unique('date'))\n",
    "\n",
    "    # Select the most traded stocks based on dollar volume\n",
    "    dollar_vol = (dataset.adjusted_close.mul(dataset.volume)\n",
    "                  .unstack('ticker')\n",
    "                  .dropna(thresh=int(.95 * n_dates), axis=1)\n",
    "                  .rank(ascending=False, axis=1)\n",
    "                  .stack('ticker'))\n",
    "\n",
    "    most_traded = dollar_vol.groupby(level='ticker').mean().nsmallest(500).index\n",
    "\n",
    "    # Calculate daily percentage returns for the most traded stocks\n",
    "    returns = (dataset.loc[idx[most_traded, :], 'adjusted_close']\n",
    "               .unstack('ticker')\n",
    "               .pct_change()\n",
    "               .sort_index(ascending=False))\n",
    "\n",
    "    # Create a sliding window of length T (21 days) and store the data in a new DataFrame\n",
    "    n = len(returns)\n",
    "    T = 21 # days\n",
    "    tcols = list(range(T))\n",
    "    tickers = returns.columns\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(n-T-1):\n",
    "        df = returns.iloc[i:i+T+1]\n",
    "        date = df.index.max()\n",
    "        data = pd.concat([data, \n",
    "                        df.reset_index(drop=True).T\n",
    "                        .assign(date=date, ticker=tickers)\n",
    "                        .set_index(['ticker', 'date'])])\n",
    "\n",
    "    # Add additional features to the data DataFrame\n",
    "    data = data.rename(columns={0: 'label'}).sort_index().dropna()\n",
    "    additional_features = ['RSI', 'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']\n",
    "    data = data.join(dataset.loc[idx[most_traded, :], additional_features])\n",
    "\n",
    "    # Clip extreme values in the dataset\n",
    "    data.loc[:, tcols[1:]] = (data.loc[:, tcols[1:]].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                    upper=x.quantile(.99))))\n",
    "    \n",
    "    # Save the preprocessed data to an HDF5 file\n",
    "    data.to_hdf('lstm_data_JDKv1.h5', 'returns_daily')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m preprocess_data_v2()\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mpreprocess_data_v2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     df \u001b[39m=\u001b[39m returns\u001b[39m.\u001b[39miloc[i:i\u001b[39m+\u001b[39mT\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m     date \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mmax()\n\u001b[0;32m---> 43\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([data, \n\u001b[1;32m     44\u001b[0m                     df\u001b[39m.\u001b[39;49mreset_index(drop\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mT\n\u001b[1;32m     45\u001b[0m                     \u001b[39m.\u001b[39;49massign(date\u001b[39m=\u001b[39;49mdate, ticker\u001b[39m=\u001b[39;49mtickers)\n\u001b[1;32m     46\u001b[0m                     \u001b[39m.\u001b[39;49mset_index([\u001b[39m'\u001b[39;49m\u001b[39mticker\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m])])\n\u001b[1;32m     48\u001b[0m \u001b[39m# Add additional features to the data DataFrame\u001b[39;00m\n\u001b[1;32m     49\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m})\u001b[39m.\u001b[39msort_index()\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m     objs,\n\u001b[1;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[1;32m    618\u001b[0m )\n\u001b[1;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[1;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/pandas/core/internals/concat.py:223\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    217\u001b[0m vals \u001b[39m=\u001b[39m [ju\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units]\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m blk\u001b[39m.\u001b[39mis_extension:\n\u001b[1;32m    220\u001b[0m     \u001b[39m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m#  than concat_compat\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(vals, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     values \u001b[39m=\u001b[39m concat_compat(vals, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = preprocess_data_v2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset provided\n",
    "dataset = pd.read_hdf('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5',\n",
    "                'stocks/prices/daily').loc[idx[:, '2013-02-15':'2023-02-15'], ['adjusted_close', 'volume', 'RSI',\n",
    "                'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]\n",
    "dataset.index.names = ['ticker', 'date']\n",
    "n_dates = len(dataset.index.unique('date'))\n",
    "\n",
    "# Select the most traded stocks based on dollar volume\n",
    "dollar_vol = (dataset.adjusted_close.mul(dataset.volume)\n",
    "              .unstack('ticker')\n",
    "              .dropna(thresh=int(.95 * n_dates), axis=1)\n",
    "              .rank(ascending=False, axis=1)\n",
    "              .stack('ticker'))\n",
    "\n",
    "most_traded = dollar_vol.groupby(level='ticker').mean().nsmallest(500).index\n",
    "\n",
    "# Calculate daily percentage returns for the most traded stocks\n",
    "returns = (dataset.loc[idx[most_traded, :], 'adjusted_close']\n",
    "           .unstack('ticker')\n",
    "           .pct_change()\n",
    "           .sort_index(ascending=False))\n",
    "\n",
    "# Create a sliding window of length T (21 days) and store the data in a new DataFrame\n",
    "n = len(returns)\n",
    "T = 21 # days\n",
    "tcols = list(range(T))\n",
    "tickers = returns.columns\n",
    "\n",
    "# Use list comprehension and concatenate the results\n",
    "data_list = [returns.iloc[i:i+T+1].reset_index(drop=True).T\n",
    "            .assign(date=returns.index[i], ticker=tickers)\n",
    "            .set_index(['ticker', 'date'])\n",
    "            for i in range(n-T-1)]\n",
    "\n",
    "data = pd.concat(data_list)\n",
    "data = data.rename(columns={0: 'label'}).sort_index().dropna()\n",
    "data.loc[:, tcols[1:]] = (data.loc[:, tcols[1:]].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                    upper=x.quantile(.99))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset provided, selects a subset of columns, and assigns names to the index.\n",
    "        \"\"\"\n",
    "        # load the dataset\n",
    "        dataset = pd.read_hdf(self.dataset_path, 'stocks/prices/daily')\n",
    "        \n",
    "        # select a subset of columns\n",
    "        dataset = dataset.loc[idx[:, '2013-02-15':'2023-02-15'], ['adjusted_close', 'volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]\n",
    "        \n",
    "        # assign names to the index\n",
    "        dataset.index.names = ['ticker', 'date']\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def select_most_traded_stocks(self, dataset):\n",
    "        \"\"\"\n",
    "        Selects the most traded stocks based on dollar volume and returns a DataFrame of their daily percentage returns.\n",
    "        \"\"\"\n",
    "        # get the number of unique dates in the dataset\n",
    "        n_dates = len(dataset.index.unique('date'))\n",
    "        \n",
    "        # calculate the dollar volume for each stock and rank them based on dollar volume\n",
    "        dollar_vol = (dataset.adjusted_close.mul(dataset.volume)\n",
    "                      .unstack('ticker')\n",
    "                      .dropna(thresh=int(.95 * n_dates), axis=1)\n",
    "                      .rank(ascending=False, axis=1)\n",
    "                      .stack('ticker'))\n",
    "\n",
    "        # select the 500 most traded stocks based on the mean rank of dollar volume\n",
    "        most_traded = dollar_vol.groupby(level='ticker').mean().nsmallest(500).index\n",
    "\n",
    "        # calculate daily percentage returns for the most traded stocks\n",
    "        returns = (dataset.loc[idx[most_traded, :], 'adjusted_close']\n",
    "                   .unstack('ticker')\n",
    "                   .pct_change()\n",
    "                   .sort_index(ascending=False))\n",
    "\n",
    "        return returns\n",
    "    \n",
    "    def create_sliding_window(self, returns, T):\n",
    "        \"\"\"\n",
    "        Creates a sliding window of length T (21 days) for the given DataFrame of returns and returns a new DataFrame.\n",
    "        \"\"\"\n",
    "        # get the number of rows in the returns DataFrame\n",
    "        n = len(returns)\n",
    "\n",
    "        # create a list of column names for the sliding window\n",
    "        tcols = list(range(T))\n",
    "\n",
    "        # get the tickers from the returns DataFrame\n",
    "        tickers = returns.columns\n",
    "\n",
    "        # create a list of DataFrames for each window\n",
    "        data_list = [returns.iloc[i:i+T+1].reset_index(drop=True).T\n",
    "                     .assign(date=returns.index[i], ticker=tickers)\n",
    "                     .set_index(['ticker', 'date'])\n",
    "                     for i in range(n-T-1)]\n",
    "\n",
    "        # concatenate the list of DataFrames into a single DataFrame\n",
    "        data = pd.concat(data_list)\n",
    "\n",
    "        # rename the first column to 'label' and drop any rows with missing values\n",
    "        data = data.rename(columns={0: 'label'}).sort_index().dropna()\n",
    "\n",
    "        # clip the values of the remaining columns to the 1st and 99th percentiles\n",
    "        data.loc[:, tcols[1:]] = (data.loc[:, tcols[1:]].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                                               upper=x.quantile(.99))))\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5')\n",
    "\n",
    "dataset = preprocessing.load_dataset()\n",
    "returns = preprocessing.select_most_traded_stocks(dataset)\n",
    "\n",
    "T = 21 # days\n",
    "\n",
    "data = preprocessing.create_sliding_window(returns, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2013-03-20</th>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.007157</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>-0.014441</td>\n",
       "      <td>-0.017902</td>\n",
       "      <td>0.017985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-21</th>\n",
       "      <td>-0.024591</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.007157</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>-0.014441</td>\n",
       "      <td>-0.017902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-22</th>\n",
       "      <td>-0.009603</td>\n",
       "      <td>-0.024591</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.007157</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>-0.014441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-25</th>\n",
       "      <td>-0.002911</td>\n",
       "      <td>-0.009603</td>\n",
       "      <td>-0.024591</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.007157</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>0.004082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-26</th>\n",
       "      <td>0.016775</td>\n",
       "      <td>-0.002911</td>\n",
       "      <td>-0.009603</td>\n",
       "      <td>-0.024591</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>-0.007157</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZTS</th>\n",
       "      <th>2023-02-09</th>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.008390</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.022933</td>\n",
       "      <td>0.049640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-10</th>\n",
       "      <td>-0.015422</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.008390</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.022933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-13</th>\n",
       "      <td>0.030380</td>\n",
       "      <td>-0.015422</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.008390</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.012729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-14</th>\n",
       "      <td>0.053696</td>\n",
       "      <td>0.030380</td>\n",
       "      <td>-0.015422</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.008390</td>\n",
       "      <td>0.006316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>0.018150</td>\n",
       "      <td>0.053696</td>\n",
       "      <td>0.030380</td>\n",
       "      <td>-0.015422</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002906</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>-0.008390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1180290 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label         1         2         3         4         5  \\\n",
       "ticker date                                                                     \n",
       "A      2013-03-20  0.016668 -0.012926 -0.010694 -0.007157  0.007441  0.008915   \n",
       "       2013-03-21 -0.024591  0.016668 -0.012926 -0.010694 -0.007157  0.007441   \n",
       "       2013-03-22 -0.009603 -0.024591  0.016668 -0.012926 -0.010694 -0.007157   \n",
       "       2013-03-25 -0.002911 -0.009603 -0.024591  0.016668 -0.012926 -0.010694   \n",
       "       2013-03-26  0.016775 -0.002911 -0.009603 -0.024591  0.016668 -0.012926   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "ZTS    2023-02-09 -0.007774 -0.021139  0.003880 -0.017104 -0.018713  0.019010   \n",
       "       2023-02-10 -0.015422 -0.007774 -0.021139  0.003880 -0.017104 -0.018713   \n",
       "       2023-02-13  0.030380 -0.015422 -0.007774 -0.021139  0.003880 -0.017104   \n",
       "       2023-02-14  0.053696  0.030380 -0.015422 -0.007774 -0.021139  0.003880   \n",
       "       2023-02-15  0.018150  0.053696  0.030380 -0.015422 -0.007774 -0.021139   \n",
       "\n",
       "                          6         7         8         9  ...        12  \\\n",
       "ticker date                                                ...             \n",
       "A      2013-03-20 -0.004437 -0.005111 -0.005085  0.000230  ...  0.002386   \n",
       "       2013-03-21  0.008915 -0.004437 -0.005111 -0.005085  ...  0.014990   \n",
       "       2013-03-22  0.007441  0.008915 -0.004437 -0.005111  ...  0.013596   \n",
       "       2013-03-25 -0.007157  0.007441  0.008915 -0.004437  ...  0.000230   \n",
       "       2013-03-26 -0.010694 -0.007157  0.007441  0.008915  ... -0.005085   \n",
       "...                     ...       ...       ...       ...  ...       ...   \n",
       "ZTS    2023-02-09  0.014019  0.004797 -0.002906 -0.018188  ... -0.005157   \n",
       "       2023-02-10  0.019010  0.014019  0.004797 -0.002906  ... -0.002291   \n",
       "       2023-02-13 -0.018713  0.019010  0.014019  0.004797  ...  0.016494   \n",
       "       2023-02-14 -0.017104 -0.018713  0.019010  0.014019  ... -0.018188   \n",
       "       2023-02-15  0.003880 -0.017104 -0.018713  0.019010  ... -0.002906   \n",
       "\n",
       "                         13        14        15        16        17        18  \\\n",
       "ticker date                                                                     \n",
       "A      2013-03-20  0.010848 -0.005993  0.018551 -0.007750 -0.012200  0.004082   \n",
       "       2013-03-21  0.002386  0.010848 -0.005993  0.018551 -0.007750 -0.012200   \n",
       "       2013-03-22  0.014990  0.002386  0.010848 -0.005993  0.018551 -0.007750   \n",
       "       2013-03-25  0.013596  0.014990  0.002386  0.010848 -0.005993  0.018551   \n",
       "       2013-03-26  0.000230  0.013596  0.014990  0.002386  0.010848 -0.005993   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "ZTS    2023-02-09  0.017948  0.011485  0.014756  0.002507 -0.008390  0.006316   \n",
       "       2023-02-10 -0.005157  0.017948  0.011485  0.014756  0.002507 -0.008390   \n",
       "       2023-02-13 -0.002291 -0.005157  0.017948  0.011485  0.014756  0.002507   \n",
       "       2023-02-14  0.016494 -0.002291 -0.005157  0.017948  0.011485  0.014756   \n",
       "       2023-02-15 -0.018188  0.016494 -0.002291 -0.005157  0.017948  0.011485   \n",
       "\n",
       "                         19        20        21  \n",
       "ticker date                                      \n",
       "A      2013-03-20 -0.014441 -0.017902  0.017985  \n",
       "       2013-03-21  0.004082 -0.014441 -0.017902  \n",
       "       2013-03-22 -0.012200  0.004082 -0.014441  \n",
       "       2013-03-25 -0.007750 -0.012200  0.004082  \n",
       "       2013-03-26  0.018551 -0.007750 -0.012200  \n",
       "...                     ...       ...       ...  \n",
       "ZTS    2023-02-09  0.012729  0.022933  0.049640  \n",
       "       2023-02-10  0.006316  0.012729  0.022933  \n",
       "       2023-02-13 -0.008390  0.006316  0.012729  \n",
       "       2023-02-14  0.002507 -0.008390  0.006316  \n",
       "       2023-02-15  0.014756  0.002507 -0.008390  \n",
       "\n",
       "[1180290 rows x 22 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_hdf('lstm_data.h5', 'returns_daily')\n",
    "\n",
    "# with pd.HDFStore('lstm_data.h5') as store:\n",
    "#     print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset provided, selects a subset of columns, and assigns names to the index.\n",
    "        \"\"\"\n",
    "        # load the dataset\n",
    "        dataset = pd.read_hdf(self.dataset_path, 'stocks/prices/daily')\n",
    "        \n",
    "        # select a subset of columns\n",
    "        dataset = dataset.loc[idx[:, '2005-02-15':'2023-02-15'], ['adjusted_close', 'volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_hist', 'BB_upper', 'BB_middle', 'BB_lower']]\n",
    "        \n",
    "        # assign names to the index\n",
    "        dataset.index.names = ['ticker', 'date']\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def select_most_traded_stocks(self, dataset):\n",
    "        \"\"\"\n",
    "        Selects the most traded stocks based on dollar volume and returns a DataFrame of their daily percentage returns.\n",
    "        \"\"\"\n",
    "        # get the number of unique dates in the dataset\n",
    "        n_dates = len(dataset.index.unique('date'))\n",
    "        \n",
    "        # calculate the dollar volume for each stock and rank them based on dollar volume\n",
    "        dollar_vol = (dataset.adjusted_close.mul(dataset.volume)\n",
    "                      .unstack('ticker')\n",
    "                      .dropna(thresh=int(.95 * n_dates), axis=1)\n",
    "                      .rank(ascending=False, axis=1)\n",
    "                      .stack('ticker'))\n",
    "\n",
    "        # select the 500 most traded stocks based on the mean rank of dollar volume\n",
    "        most_traded = dollar_vol.groupby(level='ticker').mean().nsmallest(500).index\n",
    "\n",
    "        dataset = dataset.loc[idx[most_traded, :], 'adjusted_close'].unstack('ticker')\n",
    "\n",
    "        dataset.index = pd.to_datetime(dataset.index)\n",
    "\n",
    "        # calculate weekly percentage returns for the most traded stocks\n",
    "        returns = dataset.resample('W').last().pct_change().dropna(axis=0).sort_index(ascending=False)\n",
    "\n",
    "        # convert the index to a datetime index\n",
    "        returns.index = pd.to_datetime(returns.index)\n",
    "\n",
    "        return returns\n",
    "    \n",
    "    def create_sliding_window(self, returns, T):\n",
    "        \"\"\"\n",
    "        Creates a sliding window of length T (52 weeks) for the given DataFrame of returns and returns a new DataFrame.\n",
    "        \"\"\"\n",
    "        # get the number of rows in the returns DataFrame\n",
    "        n = len(returns)\n",
    "\n",
    "        # create a list of column names for the sliding window\n",
    "        tcols = list(range(1, T+1))\n",
    "\n",
    "        # get the tickers from the returns DataFrame\n",
    "        tickers = returns.columns\n",
    "\n",
    "        # create a list of DataFrames for each window\n",
    "        data_list = [returns.iloc[i:i+T+1].reset_index(drop=True).T\n",
    "                     .assign(date=returns.index[i], ticker=tickers)\n",
    "                     .set_index(['ticker', 'date'])\n",
    "                     for i in range(n-T-1)]\n",
    "\n",
    "        # concatenate the list of DataFrames into a single DataFrame\n",
    "        data = pd.concat(data_list)\n",
    "\n",
    "        # rename the first column to 'fwd_returns' and drop any rows with missing values\n",
    "        data = data.rename(columns={0: 'fwd_returns'}).sort_index().dropna()\n",
    "\n",
    "        # clip the values of the remaining columns to the 1st and 99th percentiles\n",
    "        data.loc[:, tcols] = (data.loc[:, tcols].apply(lambda x: x.clip(lower=x.quantile(.01),\n",
    "                                                                         upper=x.quantile(.99))))\n",
    "\n",
    "        # create a new column 'label' indicating whether the forward returns are positive\n",
    "        data['label'] = (data['fwd_returns'] > 0).astype(int)\n",
    "\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5')\n",
    "\n",
    "dataset = preprocessing.load_dataset()\n",
    "\n",
    "returns = preprocessing.select_most_traded_stocks(dataset)\n",
    "\n",
    "T = 52 # weeks\n",
    "\n",
    "data = preprocessing.create_sliding_window(returns, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 354903 entries, ('A', Timestamp('2006-12-31 00:00:00')) to ('ZION', Timestamp('2023-02-19 00:00:00'))\n",
      "Data columns (total 54 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   fwd_returns  354903 non-null  float64\n",
      " 1   1            354903 non-null  float64\n",
      " 2   2            354903 non-null  float64\n",
      " 3   3            354903 non-null  float64\n",
      " 4   4            354903 non-null  float64\n",
      " 5   5            354903 non-null  float64\n",
      " 6   6            354903 non-null  float64\n",
      " 7   7            354903 non-null  float64\n",
      " 8   8            354903 non-null  float64\n",
      " 9   9            354903 non-null  float64\n",
      " 10  10           354903 non-null  float64\n",
      " 11  11           354903 non-null  float64\n",
      " 12  12           354903 non-null  float64\n",
      " 13  13           354903 non-null  float64\n",
      " 14  14           354903 non-null  float64\n",
      " 15  15           354903 non-null  float64\n",
      " 16  16           354903 non-null  float64\n",
      " 17  17           354903 non-null  float64\n",
      " 18  18           354903 non-null  float64\n",
      " 19  19           354903 non-null  float64\n",
      " 20  20           354903 non-null  float64\n",
      " 21  21           354903 non-null  float64\n",
      " 22  22           354903 non-null  float64\n",
      " 23  23           354903 non-null  float64\n",
      " 24  24           354903 non-null  float64\n",
      " 25  25           354903 non-null  float64\n",
      " 26  26           354903 non-null  float64\n",
      " 27  27           354903 non-null  float64\n",
      " 28  28           354903 non-null  float64\n",
      " 29  29           354903 non-null  float64\n",
      " 30  30           354903 non-null  float64\n",
      " 31  31           354903 non-null  float64\n",
      " 32  32           354903 non-null  float64\n",
      " 33  33           354903 non-null  float64\n",
      " 34  34           354903 non-null  float64\n",
      " 35  35           354903 non-null  float64\n",
      " 36  36           354903 non-null  float64\n",
      " 37  37           354903 non-null  float64\n",
      " 38  38           354903 non-null  float64\n",
      " 39  39           354903 non-null  float64\n",
      " 40  40           354903 non-null  float64\n",
      " 41  41           354903 non-null  float64\n",
      " 42  42           354903 non-null  float64\n",
      " 43  43           354903 non-null  float64\n",
      " 44  44           354903 non-null  float64\n",
      " 45  45           354903 non-null  float64\n",
      " 46  46           354903 non-null  float64\n",
      " 47  47           354903 non-null  float64\n",
      " 48  48           354903 non-null  float64\n",
      " 49  49           354903 non-null  float64\n",
      " 50  50           354903 non-null  float64\n",
      " 51  51           354903 non-null  float64\n",
      " 52  52           354903 non-null  float64\n",
      " 53  label        354903 non-null  int64  \n",
      "dtypes: float64(53), int64(1)\n",
      "memory usage: 147.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.get_level_values(0).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Reshape, BatchNormalization, concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "class LSTM_model:\n",
    "    def __init__(self, data, window_size=52, lstm1_units=25, lstm2_units=10, embedding_dim=5):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.lstm1_units = lstm1_units\n",
    "        self.lstm2_units = lstm2_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = None\n",
    "        self.results_path = 'results/lstm_model'\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare input data for the LSTM model and train-test split.\n",
    "        \"\"\"\n",
    "        data = self.data.copy()\n",
    "        data['ticker'] = pd.factorize(data.index.get_level_values('ticker'))[0]\n",
    "        data['month'] = data.index.get_level_values('date').month\n",
    "        data = pd.get_dummies(data, columns=['month'], prefix='month')\n",
    "        \n",
    "        window_size = self.window_size\n",
    "        sequence = list(range(1, window_size+1))\n",
    "        ticker = 1\n",
    "        months = 12\n",
    "        n_tickers = data.ticker.nunique()\n",
    "        \n",
    "        train_data = data.loc[idx[:, :'2016'], :]\n",
    "        test_data = data.loc[idx[:, '2017'],:]\n",
    "        \n",
    "        X_train = [\n",
    "            train_data.loc[:, sequence].values.reshape(-1, window_size , 1),\n",
    "            train_data.ticker,\n",
    "            train_data.filter(like='month')\n",
    "        ]\n",
    "        y_train = train_data.fwd_returns\n",
    "        \n",
    "        X_test = [\n",
    "            test_data.loc[:, list(range(1, window_size+1))].values.reshape(-1, window_size , 1),\n",
    "            test_data.ticker,\n",
    "            test_data.filter(like='month')\n",
    "        ]\n",
    "        y_test = test_data.fwd_returns\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create the LSTM model architecture.\n",
    "        \"\"\"\n",
    "        K.clear_session()\n",
    "        n_features = 1\n",
    "        window_size = self.window_size\n",
    "        n_tickers = self.data.index.get_level_values(0).nunique()\n",
    "        \n",
    "        # Input Layers\n",
    "        returns = Input(shape=(window_size, n_features), name='Returns')\n",
    "        tickers = Input(shape=(1,), name='Tickers')\n",
    "        months = Input(shape=(12,), name='Months')\n",
    "        \n",
    "        # LSTM Layers\n",
    "        lstm1 = LSTM(units=self.lstm1_units, \n",
    "                     input_shape=(window_size, n_features), \n",
    "                     name='LSTM1', \n",
    "                     dropout=.2,\n",
    "                     return_sequences=True)(returns)\n",
    "        lstm_model = LSTM(units=self.lstm2_units, \n",
    "                          dropout=.2,\n",
    "                          name='LSTM2')(lstm1)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        ticker_embedding = Embedding(input_dim=n_tickers, \n",
    "                                     output_dim=self.embedding_dim, \n",
    "                                     input_length=1)(tickers)\n",
    "        ticker_embedding = Reshape(target_shape=(self.embedding_dim,))(ticker_embedding)\n",
    "        \n",
    "        # Concatenate Model components\n",
    "        merged = concatenate([lstm_model, ticker_embedding, months], name='Merged')\n",
    "        bn = BatchNormalization()(merged)\n",
    "        hidden_dense = Dense(10, name='FC1')(bn)\n",
    "        output = Dense(1, name='Output')(hidden_dense)\n",
    "        \n",
    "        # Create and compile the model\n",
    "        self.model = Model(inputs=[returns, tickers, months], outputs=output)\n",
    "        print(self.model.summary())\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_test, y_test, epochs=50, batch_size=64, save_best_only=True, early_stopping_patience=5):\n",
    "        \"\"\"\n",
    "        Train the LSTM model using the prepared input data.\n",
    "        \"\"\"\n",
    "        # Create directory to save model if it does not exist\n",
    "        Path(self.results_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Filepath to save the best model\n",
    "        lstm_path = f'{self.results_path}/lstm.regression.h5'\n",
    "\n",
    "        # Model checkpoint callback to save the best model based on validation loss\n",
    "        checkpointer = ModelCheckpoint(filepath=lstm_path,\n",
    "                                    verbose=1,\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    save_best_only=save_best_only)\n",
    "\n",
    "        # Early stopping callback to stop training if validation loss does not improve after certain number of epochs\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                    patience=early_stopping_patience,\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        training = self.model.fit(X_train,\n",
    "                                y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_data=(X_test, y_test),\n",
    "                                callbacks=[early_stopping, checkpointer],\n",
    "                                verbose=1)\n",
    "        \n",
    "        return training\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test dataset and return predictions.\n",
    "        \"\"\"\n",
    "        # Load the best model saved during training\n",
    "        self.model.load_weights(f'{self.results_path}/lstm.regression.h5')\n",
    "        \n",
    "        # Get model predictions on the test dataset\n",
    "        test_predict = pd.Series(self.model.predict(X_test).squeeze(), index=y_test.index)\n",
    "        \n",
    "        return test_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 20:22:49.272462: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:49.273532: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:49.275626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-06 20:22:49.422017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:49.423624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:49.425233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Returns (InputLayer)           [(None, 52, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " Tickers (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " LSTM1 (LSTM)                   (None, 52, 25)       2700        ['Returns[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 5)         2105        ['Tickers[0][0]']                \n",
      "                                                                                                  \n",
      " LSTM2 (LSTM)                   (None, 10)           1440        ['LSTM1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 5)            0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " Months (InputLayer)            [(None, 12)]         0           []                               \n",
      "                                                                                                  \n",
      " Merged (Concatenate)           (None, 27)           0           ['LSTM2[0][0]',                  \n",
      "                                                                  'reshape[0][0]',                \n",
      "                                                                  'Months[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 27)          108         ['Merged[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " FC1 (Dense)                    (None, 10)           280         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " Output (Dense)                 (None, 1)            11          ['FC1[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,644\n",
      "Trainable params: 6,590\n",
      "Non-trainable params: 54\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 20:22:49.832717: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:49.834017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:49.835308: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-06 20:22:49.998646: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:49.999873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:50.000783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-06 20:22:50.848963: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:50.850082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:50.851235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-06 20:22:51.003693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-06 20:22:51.004708: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-06 20:22:51.005802: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304/3434 [==========>...................] - ETA: 1:09 - loss: 0.0163"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m lstm_model\u001b[39m.\u001b[39mcreate_model()\n\u001b[1;32m     18\u001b[0m \u001b[39m#Train the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m training \u001b[39m=\u001b[39m lstm_model\u001b[39m.\u001b[39;49mtrain_model(X_train, y_train, X_test, y_test)\n\u001b[1;32m     21\u001b[0m \u001b[39m#Evaluate the model and get predictions on the test dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m test_predict \u001b[39m=\u001b[39m lstm_model\u001b[39m.\u001b[39mevaluate_model(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[9], line 117\u001b[0m, in \u001b[0;36mLSTM_model.train_model\u001b[0;34m(self, X_train, y_train, X_test, y_test, epochs, batch_size, save_best_only, early_stopping_patience)\u001b[0m\n\u001b[1;32m    112\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m    113\u001b[0m                             patience\u001b[39m=\u001b[39mearly_stopping_patience,\n\u001b[1;32m    114\u001b[0m                             restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X_train,\n\u001b[1;32m    118\u001b[0m                         y_train,\n\u001b[1;32m    119\u001b[0m                         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    120\u001b[0m                         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    121\u001b[0m                         validation_data\u001b[39m=\u001b[39;49m(X_test, y_test),\n\u001b[1;32m    122\u001b[0m                         callbacks\u001b[39m=\u001b[39;49m[early_stopping, checkpointer],\n\u001b[1;32m    123\u001b[0m                         verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m training\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create an instance of the Preprocessing class\n",
    "preprocessing = Preprocessing('/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5')\n",
    "\n",
    "#Load and preprocess the dataset\n",
    "dataset = preprocessing.load_dataset()\n",
    "returns = preprocessing.select_most_traded_stocks(dataset)\n",
    "data = preprocessing.create_sliding_window(returns, T=52)\n",
    "\n",
    "#Create an instance of the LSTM_model class\n",
    "lstm_model = LSTM_model(data)\n",
    "\n",
    "#Prepare the input data for training\n",
    "X_train, y_train, X_test, y_test = lstm_model.prepare_data()\n",
    "\n",
    "#reate the LSTM model architecture\n",
    "lstm_model.create_model()\n",
    "\n",
    "#Train the model\n",
    "training = lstm_model.train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#Evaluate the model and get predictions on the test dataset\n",
    "test_predict = lstm_model.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig, Dataset\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Load the workspace\n",
    "workspace = Workspace.from_config()\n",
    "\n",
    "# Get the compute target\n",
    "compute_target = ComputeTarget(workspace=workspace, name='your-compute-cluster')\n",
    "\n",
    "# Get the dataset\n",
    "dataset = Dataset.get_by_name(workspace, 'your-dataset-name')\n",
    "\n",
    "# Define the training environment\n",
    "env = Environment('lstm-env')\n",
    "env.python.conda_dependencies.add_pip_package('tensorflow')\n",
    "env.python.conda_dependencies.add_pip_package('pandas')\n",
    "env.python.conda_dependencies.add_pip_package('numpy')\n",
    "env.python.conda_dependencies.add_pip_package('h5py')\n",
    "\n",
    "# Define the script run configuration\n",
    "src = ScriptRunConfig(source_directory='path-to-training-script',\n",
    "                      script='train.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env,\n",
    "                      arguments=['--data-path', dataset.as_named_input('input').as_mount()])\n",
    "\n",
    "# Submit the experiment\n",
    "experiment = Experiment(workspace=workspace, name='lstm-training')\n",
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Upload the dataset file to the Blob Storage\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(local_file_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m data:\n\u001b[0;32m---> 18\u001b[0m     container_client\u001b[39m.\u001b[39;49mupload_blob(blob_file_name, data, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_container_client.py:1089\u001b[0m, in \u001b[0;36mContainerClient.upload_blob\u001b[0;34m(self, name, data, blob_type, length, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m timeout \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1088\u001b[0m encoding \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUTF-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1089\u001b[0m blob\u001b[39m.\u001b[39;49mupload_blob(\n\u001b[1;32m   1090\u001b[0m     data,\n\u001b[1;32m   1091\u001b[0m     blob_type\u001b[39m=\u001b[39;49mblob_type,\n\u001b[1;32m   1092\u001b[0m     length\u001b[39m=\u001b[39;49mlength,\n\u001b[1;32m   1093\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m   1094\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1095\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1096\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1097\u001b[0m )\n\u001b[1;32m   1098\u001b[0m \u001b[39mreturn\u001b[39;00m blob\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_blob_client.py:746\u001b[0m, in \u001b[0;36mBlobClient.upload_blob\u001b[0;34m(self, data, blob_type, length, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upload_blob_options(\n\u001b[1;32m    740\u001b[0m     data,\n\u001b[1;32m    741\u001b[0m     blob_type\u001b[39m=\u001b[39mblob_type,\n\u001b[1;32m    742\u001b[0m     length\u001b[39m=\u001b[39mlength,\n\u001b[1;32m    743\u001b[0m     metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[1;32m    744\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    745\u001b[0m \u001b[39mif\u001b[39;00m blob_type \u001b[39m==\u001b[39m BlobType\u001b[39m.\u001b[39mBlockBlob:\n\u001b[0;32m--> 746\u001b[0m     \u001b[39mreturn\u001b[39;00m upload_block_blob(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    747\u001b[0m \u001b[39mif\u001b[39;00m blob_type \u001b[39m==\u001b[39m BlobType\u001b[39m.\u001b[39mPageBlob:\n\u001b[1;32m    748\u001b[0m     \u001b[39mreturn\u001b[39;00m upload_page_blob(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_upload_helpers.py:152\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[0;34m(client, data, stream, length, overwrite, headers, validate_content, max_concurrency, blob_settings, encryption_options, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[39m# V2 wraps the data stream with an encryption stream\u001b[39;00m\n\u001b[1;32m    150\u001b[0m             stream \u001b[39m=\u001b[39m GCMBlobEncryptionStream(cek, stream)\n\u001b[0;32m--> 152\u001b[0m     block_ids \u001b[39m=\u001b[39m upload_data_chunks(\n\u001b[1;32m    153\u001b[0m         service\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m    154\u001b[0m         uploader_class\u001b[39m=\u001b[39;49mBlockBlobChunkUploader,\n\u001b[1;32m    155\u001b[0m         total_size\u001b[39m=\u001b[39;49mtotal_size,\n\u001b[1;32m    156\u001b[0m         chunk_size\u001b[39m=\u001b[39;49mblob_settings\u001b[39m.\u001b[39;49mmax_block_size,\n\u001b[1;32m    157\u001b[0m         max_concurrency\u001b[39m=\u001b[39;49mmax_concurrency,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         validate_content\u001b[39m=\u001b[39;49mvalidate_content,\n\u001b[1;32m    160\u001b[0m         progress_hook\u001b[39m=\u001b[39;49mprogress_hook,\n\u001b[1;32m    161\u001b[0m         encryptor\u001b[39m=\u001b[39;49mencryptor,\n\u001b[1;32m    162\u001b[0m         padder\u001b[39m=\u001b[39;49mpadder,\n\u001b[1;32m    163\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    164\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     block_ids \u001b[39m=\u001b[39m upload_substream_blocks(\n\u001b[1;32m    168\u001b[0m         service\u001b[39m=\u001b[39mclient,\n\u001b[1;32m    169\u001b[0m         uploader_class\u001b[39m=\u001b[39mBlockBlobChunkUploader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/uploads.py:78\u001b[0m, in \u001b[0;36mupload_data_chunks\u001b[0;34m(service, uploader_class, total_size, chunk_size, max_concurrency, stream, validate_content, progress_hook, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         range_ids \u001b[39m=\u001b[39m _parallel_uploads(executor, uploader\u001b[39m.\u001b[39mprocess_chunk, upload_tasks, running_futures)\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     range_ids \u001b[39m=\u001b[39m [uploader\u001b[39m.\u001b[39mprocess_chunk(result) \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m uploader\u001b[39m.\u001b[39mget_chunk_streams()]\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(range_ids):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m [r[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(range_ids, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m r: r[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/uploads.py:78\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         range_ids \u001b[39m=\u001b[39m _parallel_uploads(executor, uploader\u001b[39m.\u001b[39mprocess_chunk, upload_tasks, running_futures)\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     range_ids \u001b[39m=\u001b[39m [uploader\u001b[39m.\u001b[39;49mprocess_chunk(result) \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m uploader\u001b[39m.\u001b[39mget_chunk_streams()]\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(range_ids):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m [r[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(range_ids, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m r: r[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/uploads.py:194\u001b[0m, in \u001b[0;36m_ChunkUploader.process_chunk\u001b[0;34m(self, chunk_data)\u001b[0m\n\u001b[1;32m    192\u001b[0m chunk_bytes \u001b[39m=\u001b[39m chunk_data[\u001b[39m1\u001b[39m]\n\u001b[1;32m    193\u001b[0m chunk_offset \u001b[39m=\u001b[39m chunk_data[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 194\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upload_chunk_with_progress(chunk_offset, chunk_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/uploads.py:210\u001b[0m, in \u001b[0;36m_ChunkUploader._upload_chunk_with_progress\u001b[0;34m(self, chunk_offset, chunk_data)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_upload_chunk_with_progress\u001b[39m(\u001b[39mself\u001b[39m, chunk_offset, chunk_data):\n\u001b[0;32m--> 210\u001b[0m     range_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upload_chunk(chunk_offset, chunk_data)\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_progress(\u001b[39mlen\u001b[39m(chunk_data))\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m range_id\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/uploads.py:259\u001b[0m, in \u001b[0;36mBlockBlobChunkUploader._upload_chunk\u001b[0;34m(self, chunk_offset, chunk_data)\u001b[0m\n\u001b[1;32m    257\u001b[0m index \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mchunk_offset\u001b[39m:\u001b[39;00m\u001b[39m032d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    258\u001b[0m block_id \u001b[39m=\u001b[39m encode_base64(url_quote(encode_base64(index)))\n\u001b[0;32m--> 259\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mstage_block(\n\u001b[1;32m    260\u001b[0m     block_id,\n\u001b[1;32m    261\u001b[0m     \u001b[39mlen\u001b[39;49m(chunk_data),\n\u001b[1;32m    262\u001b[0m     chunk_data,\n\u001b[1;32m    263\u001b[0m     data_stream_total\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtotal_size,\n\u001b[1;32m    264\u001b[0m     upload_stream_current\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogress_total,\n\u001b[1;32m    265\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_options\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m index, block_id\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_generated/operations/_block_blob_operations.py:1222\u001b[0m, in \u001b[0;36mBlockBlobOperations.stage_block\u001b[0;34m(self, block_id, content_length, body, transactional_content_md5, transactional_content_crc64, timeout, request_id_parameter, lease_access_conditions, cpk_info, cpk_scope_info, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m request \u001b[39m=\u001b[39m _convert_request(request)\n\u001b[1;32m   1220\u001b[0m request\u001b[39m.\u001b[39murl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mformat_url(request\u001b[39m.\u001b[39murl)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1222\u001b[0m pipeline_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_pipeline\u001b[39m.\u001b[39;49mrun(  \u001b[39m# type: ignore # pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m     request, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1224\u001b[0m )\n\u001b[1;32m   1226\u001b[0m response \u001b[39m=\u001b[39m pipeline_response\u001b[39m.\u001b[39mhttp_response\n\u001b[1;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m201\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:205\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[39m=\u001b[39m PipelineRequest(\n\u001b[1;32m    198\u001b[0m     request, context\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m first_node \u001b[39m=\u001b[39m (\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies[\u001b[39m0\u001b[39m]\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies\n\u001b[1;32m    203\u001b[0m     \u001b[39melse\u001b[39;00m _TransportRunner(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport)\n\u001b[1;32m    204\u001b[0m )\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m first_node\u001b[39m.\u001b[39;49msend(pipeline_request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 69 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/policies/_redirect.py:160\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    158\u001b[0m redirect_settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_redirects(request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions)\n\u001b[1;32m    159\u001b[0m \u001b[39mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 160\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    161\u001b[0m     redirect_location \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m redirect_location \u001b[39mand\u001b[39;00m redirect_settings[\u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:520\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mwhile\u001b[39;00m retries_remaining:\n\u001b[1;32m    519\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    521\u001b[0m         \u001b[39mif\u001b[39;00m is_retry(response, retry_settings[\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    522\u001b[0m             retries_remaining \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincrement(\n\u001b[1;32m    523\u001b[0m                 retry_settings,\n\u001b[1;32m    524\u001b[0m                 request\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mhttp_request,\n\u001b[1;32m    525\u001b[0m                 response\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mhttp_response)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 69 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/policies.py:313\u001b[0m, in \u001b[0;36mStorageResponseHook.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    308\u001b[0m     upload_stream_current \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mupload_stream_current\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m response_callback \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mresponse_callback\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    311\u001b[0m     request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mraw_response_hook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_callback)\n\u001b[0;32m--> 313\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    315\u001b[0m will_retry \u001b[39m=\u001b[39m is_retry(response, request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    316\u001b[0m \u001b[39m# Auth error could come from Bearer challenge, in which case this request will be made again\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:69\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     67\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/_base.py:100\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[1;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m    :param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m     99\u001b[0m         request\u001b[39m.\u001b[39mhttp_request,\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sender\u001b[39m.\u001b[39;49msend(request\u001b[39m.\u001b[39;49mhttp_request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49moptions),\n\u001b[1;32m    101\u001b[0m         context\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mcontext,\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/base_client.py:329\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transport\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/storage/blob/_shared/base_client.py:329\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transport\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azure/core/pipeline/transport/_requests_basic.py:338\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         read_timeout \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mread_timeout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection_config\u001b[39m.\u001b[39mread_timeout\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m         timeout \u001b[39m=\u001b[39m (connection_timeout, read_timeout)\n\u001b[0;32m--> 338\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    339\u001b[0m         request\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    340\u001b[0m         request\u001b[39m.\u001b[39;49murl,\n\u001b[1;32m    341\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    342\u001b[0m         data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    343\u001b[0m         files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[1;32m    344\u001b[0m         verify\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mconnection_verify\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection_config\u001b[39m.\u001b[39;49mverify),\n\u001b[1;32m    345\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    346\u001b[0m         cert\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mconnection_cert\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection_config\u001b[39m.\u001b[39;49mcert),\n\u001b[1;32m    347\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    348\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m     response\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39menforce_content_length \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m    353\u001b[0m     urllib3\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mNewConnectionError,\n\u001b[1;32m    354\u001b[0m     urllib3\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeoutError,\n\u001b[1;32m    355\u001b[0m ) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    396\u001b[0m         conn\u001b[39m.\u001b[39mrequest_chunked(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (six\u001b[39m.\u001b[39mensure_str(k\u001b[39m.\u001b[39mlower()) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m headers):\n\u001b[1;32m    243\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1283\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1328\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/http/client.py:1079\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1076\u001b[0m         \u001b[39m# chunked encoding\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m         chunk \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunk)\u001b[39m:\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m chunk \\\n\u001b[1;32m   1078\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1079\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(chunk)\n\u001b[1;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# end chunked transfer\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/http/client.py:1001\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock\u001b[39m.\u001b[39;49msendall(data)\n\u001b[1;32m   1002\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/ssl.py:1205\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         amount \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(byte_view)\n\u001b[1;32m   1204\u001b[0m         \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m amount:\n\u001b[0;32m-> 1205\u001b[0m             v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(byte_view[count:])\n\u001b[1;32m   1206\u001b[0m             count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m v\n\u001b[1;32m   1207\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/ssl.py:1174\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1171\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1172\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1173\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mwrite(data)\n\u001b[1;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msend(data, flags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "storage_account_name = \"atsmain\"\n",
    "storage_account_key = \"eFG+qDGbyl8dzRAXJeOlaqAZgOvNGwpennSLCnqgFT91y83dBfcUwcEjPS3FJgujHtazEWbylIb++AStIdsfbw==\"\n",
    "container_name = \"test\"\n",
    "local_file_path = '/home/groovyjac/projects/autonomous-portfolio-management/main_data_store_JDKv1.h5'\n",
    "blob_file_name = \"main_data_store_JDKv1.h5\"\n",
    "\n",
    "# Create a BlobServiceClient\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get the container client\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Upload the dataset file to the Blob Storage\n",
    "with open(local_file_path, \"rb\") as data:\n",
    "    container_client.upload_blob(blob_file_name, data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Linux distribution ubuntu 22.04 does not have automatic support. \nMissing packages: {'liblttng-ust.so.0'}\n.NET Core 3.1 can still be used via `dotnetcore2` if the required dependencies are installed.\nVisit https://aka.ms/dotnet-install-linux for Linux distro specific .NET Core install instructions.\nFollow your distro specific instructions to install `dotnet-runtime-*` and replace `*` with `3.1.23`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/dotnetcore2/runtime.py:271\u001b[0m, in \u001b[0;36mensure_dependencies.<locals>.attempt_get_deps\u001b[0;34m(missing_pkgs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     blob_deps_to_file()\n\u001b[1;32m    272\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/dotnetcore2/runtime.py:263\u001b[0m, in \u001b[0;36mensure_dependencies.<locals>.blob_deps_to_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m ssl_context \u001b[39m=\u001b[39m ssl\u001b[39m.\u001b[39mcreate_default_context(cafile\u001b[39m=\u001b[39mcafile)\n\u001b[0;32m--> 263\u001b[0m blob \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39;49murlopen(deps_url, context\u001b[39m=\u001b[39;49mssl_context)\n\u001b[1;32m    264\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(deps_tar_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    522\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    633\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    560\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    493\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    495\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Register the dataset\u001b[39;00m\n\u001b[1;32m     15\u001b[0m datastore_path \u001b[39m=\u001b[39m [(datastore, blob_file_name)]\n\u001b[0;32m---> 16\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mFile\u001b[39m.\u001b[39;49mfrom_files(datastore_path)\n\u001b[1;32m     17\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mregister(workspace, \u001b[39m'\u001b[39m\u001b[39mmain_data_store_JDKv1\u001b[39m\u001b[39m'\u001b[39m, create_new_version\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    133\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(al, \u001b[39m'\u001b[39m\u001b[39mactivity_info\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/data/dataset_factory.py:892\u001b[0m, in \u001b[0;36mFileDatasetFactory.from_files\u001b[0;34m(path, validate, partition_format, is_file)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[39mraise\u001b[39;00m UserErrorException(\u001b[39m'\u001b[39m\u001b[39mNewest version of azureml-dataprep must be installed to use is_file.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m     dataflow \u001b[39m=\u001b[39m dataprep()\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mdataflow\u001b[39m.\u001b[39;49mDataflow\u001b[39m.\u001b[39;49m_path_to_get_files_block(path)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m partition_format:\n\u001b[1;32m    895\u001b[0m     dataflow \u001b[39m=\u001b[39m _transform_and_validate(\n\u001b[1;32m    896\u001b[0m         dataflow, partition_format, \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    897\u001b[0m         validate)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/dataflow.py:2498\u001b[0m, in \u001b[0;36mDataflow._path_to_get_files_block\u001b[0;34m(path, archive_options)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2497\u001b[0m     \u001b[39mif\u001b[39;00m _is_datapath(path) \u001b[39mor\u001b[39;00m _is_datapaths(path):\n\u001b[0;32m-> 2498\u001b[0m         \u001b[39mreturn\u001b[39;00m datastore_to_dataflow(path)\n\u001b[1;32m   2499\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2500\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/_datastore_helper.py:39\u001b[0m, in \u001b[0;36mdatastore_to_dataflow\u001b[0;34m(data_source, query_timeout, is_file)\u001b[0m\n\u001b[1;32m     37\u001b[0m datastore_values \u001b[39m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m source \u001b[39min\u001b[39;00m data_source:\n\u001b[0;32m---> 39\u001b[0m     datastore, datastore_value \u001b[39m=\u001b[39m get_datastore_value(source)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fs_datastore(datastore):\n\u001b[1;32m     41\u001b[0m         \u001b[39mraise\u001b[39;00m NotSupportedDatastoreTypeError(datastore)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/_datastore_helper.py:91\u001b[0m, in \u001b[0;36mget_datastore_value\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m     88\u001b[0m _ensure_supported(datastore)\n\u001b[1;32m     90\u001b[0m workspace \u001b[39m=\u001b[39m datastore\u001b[39m.\u001b[39mworkspace\n\u001b[0;32m---> 91\u001b[0m _set_auth_type(workspace)\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m (datastore, DatastoreValue(\n\u001b[1;32m     93\u001b[0m     subscription\u001b[39m=\u001b[39mworkspace\u001b[39m.\u001b[39msubscription_id,\n\u001b[1;32m     94\u001b[0m     resource_group\u001b[39m=\u001b[39mworkspace\u001b[39m.\u001b[39mresource_group,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     path\u001b[39m=\u001b[39mpath_on_storage\n\u001b[1;32m     98\u001b[0m ))\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/_datastore_helper.py:177\u001b[0m, in \u001b[0;36m_set_auth_type\u001b[0;34m(workspace)\u001b[0m\n\u001b[1;32m    174\u001b[0m     auth_type \u001b[39m=\u001b[39m AuthType\u001b[39m.\u001b[39mDERIVED\n\u001b[1;32m    176\u001b[0m auth_value \u001b[39m=\u001b[39m auth\n\u001b[0;32m--> 177\u001b[0m get_engine_api()\u001b[39m.\u001b[39mset_aml_auth(SetAmlAuthMessageArgument(auth_type, json\u001b[39m.\u001b[39mdumps(auth_value)))\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/engineapi/api.py:19\u001b[0m, in \u001b[0;36mget_engine_api\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mglobal\u001b[39;00m _engine_api\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _engine_api:\n\u001b[0;32m---> 19\u001b[0m     _engine_api \u001b[39m=\u001b[39m EngineAPI()\n\u001b[1;32m     21\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dataset_resolver\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset_resolver\n\u001b[1;32m     22\u001b[0m     register_dataset_resolver(_engine_api\u001b[39m.\u001b[39mrequests_channel)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/engineapi/api.py:102\u001b[0m, in \u001b[0;36mEngineAPI.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine_server_secret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_host_secret(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequests_channel\u001b[39m.\u001b[39mhost_secret)\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine_server_port \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_host_channel_port(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequests_channel\u001b[39m.\u001b[39mport)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_channel \u001b[39m=\u001b[39m launch_engine()\n\u001b[1;32m    103\u001b[0m connect_to_requests_channel()\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_channel\u001b[39m.\u001b[39mon_relaunch(connect_to_requests_channel)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/azureml/dataprep/api/engineapi/engine.py:333\u001b[0m, in \u001b[0;36mlaunch_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m    331\u001b[0m engine_path \u001b[39m=\u001b[39m _get_engine_path()\n\u001b[1;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     dependencies_path \u001b[39m=\u001b[39m runtime\u001b[39m.\u001b[39;49mensure_dependencies()\n\u001b[1;32m    334\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    335\u001b[0m     _LoggerFactory\u001b[39m.\u001b[39mtrace(log, \u001b[39m'\u001b[39m\u001b[39mFailed to ensure dependencies\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/dotnetcore2/runtime.py:285\u001b[0m, in \u001b[0;36mensure_dependencies\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m         success \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m success\n\u001b[0;32m--> 285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attempt_get_deps(missing_pkgs):\n\u001b[1;32m    286\u001b[0m     \u001b[39m# Failed accessing blob, likely an interrupted connection. Try again once more.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attempt_get_deps(missing_pkgs):\n\u001b[1;32m    288\u001b[0m         err_msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mUnable to retrieve .NET dependencies. Please make sure you are connected to the Internet and have \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m            a stable network connection. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMissing packages: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(missing_pkgs, _unsupported_help_msg)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureml_test/lib/python3.9/site-packages/dotnetcore2/runtime.py:279\u001b[0m, in \u001b[0;36mensure_dependencies.<locals>.attempt_get_deps\u001b[0;34m(missing_pkgs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mcode \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[1;32m    276\u001b[0m         \u001b[39m# Requested blob not found so we don't have deps for this distribution.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m         err_msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLinux distribution \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m does not have automatic support. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMissing packages: \u001b[39m\u001b[39m{3}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    278\u001b[0m             dist, version[\u001b[39m0\u001b[39m], version[\u001b[39m1\u001b[39m], missing_pkgs)\n\u001b[0;32m--> 279\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(err_msg \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m _unsupported_help_msg)\n\u001b[1;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mException when accessing blob: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Linux distribution ubuntu 22.04 does not have automatic support. \nMissing packages: {'liblttng-ust.so.0'}\n.NET Core 3.1 can still be used via `dotnetcore2` if the required dependencies are installed.\nVisit https://aka.ms/dotnet-install-linux for Linux distro specific .NET Core install instructions.\nFollow your distro specific instructions to install `dotnet-runtime-*` and replace `*` with `3.1.23`.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "# Replace these variables with your own values\n",
    "subscription_id = \"3bc70aff-a4bc-47bc-9782-28269ac52dfa\"\n",
    "resource_group = \"autonomous-portfolio-management\"\n",
    "workspace_name = \"ats-lstm\"\n",
    "\n",
    "# Load your workspace\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "# Get the datastore\n",
    "datastore = Datastore.get(workspace, storage_account_name)\n",
    "\n",
    "# Register the dataset\n",
    "datastore_path = [(datastore, blob_file_name)]\n",
    "dataset = Dataset.File.from_files(datastore_path)\n",
    "dataset = dataset.register(workspace, 'main_data_store_JDKv1', create_new_version=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
